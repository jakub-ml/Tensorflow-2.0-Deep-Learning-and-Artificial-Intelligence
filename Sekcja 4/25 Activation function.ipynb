{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYg5sIW6v2rPzFZpxLJlmK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Sigmoid\n","### **Sigmoid function** - kiedyś bardzo popularna funkcja aktywacji ze względu na wysoką skuteczność, z czasem fakt tego że wartość oczekiwana tej funkcji jest równa 0.5 przez rozkład wartości przedstawiony na wykresie, dane przekazywane do optymalizacji nie były idealnie ustandaryzowane, najlepsze dla optyzalizacji jest to żeby wartość oczekiwana była równa 0 a wartości były w bliskiej odległości.\n","![picture](https://drive.google.com/uc?id=19LFavYFqNuwEpQuZtApydbR6I1Vk-AlZ&usp)\n","![picture](https://drive.google.com/uc?id=1biVN6FdxDQnbYnW5_Kl1XEUa37m9PGT2&usp)\n","\n"],"metadata":{"id":"AhYkVyk4Kx5x"}},{"cell_type":"markdown","source":["# Tanh\n","### **Tanh function** - ta funkcja nie ma takiego problemu jak poprzednia, wartość oczekiwana jest równa 0, dzięki temu optymalizacja działa lepiej.\n","\n","![picture](https://drive.google.com/uc?id=1bE4mCrkdvFd_g3BDiMFup6RVRhMig32q&usp)\n","\n","\n","### Problemem z Sigmoidem i Tanh jest **\"The Vanishing Gradient Problem\"**. Polega to na tym że pochodna funkcji Tanh to 0.25, jeśli mamy sieć neuronową która ma 5 warstw to każda za każdym razem do tej wartości musimy zastosować do tej wartości funkcje aktywacji, z tego powodu wartość ta dla ostatniej warstwy może być równa przykłądowo 0.0001, co sprawia że te warstwy uczą się mało skutecznie, im szersza sieć neurnowa tym większy problem. Powoduje to że gradient jest w pewnym momencie bliski 0, przez co model sam w sobie nie poprawia w znaczym spotniu swojej skuteczności, im bliżej jesteśmy warstwy wejściowej tym bardziej spotengowany jest problem. Z tego powodu nie budowano bardzo głębokich sieci neuronowych.\n","\n","![picture](https://drive.google.com/uc?id=1weAQ59DK9ZK9KvdpMDS7ZvmQpT2jpI0n&usp)\n","\n","\n","\n"],"metadata":{"id":"8KH2oiF8M6zv"}},{"cell_type":"markdown","source":["# Relu\n","### **Relu function** - ta funkcja nie ma problemu z \"The vanishing gradient problem\". Ponieważ wartość pochodnej nie znika na krańcach przedziału.\"\n","\n","![picture](https://drive.google.com/uc?id=1cNBkS9ub8lKdAhg3vJ1cMB9xOdfl8Ovy&usp)\n","\n","### Wartości ujemne są zrównane do 0, a dodatnie są niezmieniane. Przez to że wartości ujemne są \"usuwane\", tracimy pewne informacje. Tworzy to  **dead neurons** czyli neurony które są niektywne ponieważ ich wartości są ujemne, a więc nie dostarczają cennych informacji. Można pozbyć się tego problemu stosując pewne modyfikacje dla funkcji Relu.\n","\n","![picture](https://drive.google.com/uc?id=1TZBnRW7lulpL9KW4UOYu-d4ZnDfrq1Qk&usp)\n","\n"],"metadata":{"id":"W6F0k4CmXEKn"}},{"cell_type":"code","source":["disney r&d imaginering"],"metadata":{"id":"xqxuXLRaYyvb"},"execution_count":null,"outputs":[]}]}