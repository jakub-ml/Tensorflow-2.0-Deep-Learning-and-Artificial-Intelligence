{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMT9iDjFW8Iw9OuHqNSEyV6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Gradient Descent vs Stochastic Gradient Descent\n","- **gradient descent** - cały dataset wstawiamy do epochy, w ten sposób za każdym razem w epoce mamy te same dane, powoduje to że trwa to bardzo długo w przypadku dużej ilości danych. Czasem dataset jest na tyle duży że nie możemy władować go całego do pamięci.\n","- **stochastic gradient descent** - w kazdej epoce uczymy model fragmentami datasetu dlatego każda epocha jest uczona przez liczbe danych równy liczbie batch_size, np 32, 64, 128. Uczenie epochi jest szybsze, jeśli dataset jest duzy to dzięki tej metodzie dlatego że możemy załadować dane które w innym przypadku by się nie zmieściły w pamięci.\n"],"metadata":{"id":"T27st4N5uLzg"}},{"cell_type":"markdown","source":["# Learning rate scheduling\n","Polega na powolnym zmiejszaniu learning ratu w celu uniknięcia \"przeskakiwania\" nad minimum.\n","![picture](https://drive.google.com/uc?id=1T9ngRvQxhcbtT6sBvzGxrIkwrda1fGJh)\n","\n","### Step decay\n","W tym przypadku zmieniszamy learning rate przykładowo o połowe  co 100 epoch.\n","![picture](https://drive.google.com/uc?id=1jdXWVUyXdrcYLmq-z2Bu4ycd7s613T19)\n","\n","### Exponential decay\n","W tym przypadku zmiejszamy co epoche learning rate zgodnie w funkcją\n","![picture](https://drive.google.com/uc?id=1eGYwtXUI3znT2E9-5kpwDD3-fpgKOo_l)\n","\n","### Babysitting method decay\n","W tym przypadku ustawiamy ręcznie w jakich miejscach chcemy zmniejszyć learning rate polegając na naszej znajomości danych i tego gdzie uważamy że zadziała to najlepiej\n","![picture](https://drive.google.com/uc?id=1q6CNrQPCpWzZI_5EyUqx_IoT7Ifo9tQ3)\n","\n","# Adaptive Learning Rate\n","Polega na dostosowaniu learning ratu dla każdej cechy z osobna, robimy to dlatego że jesli jakaś cecha ma mniejszy wpływ na loss function to nie chcemy żeby negatywnie wplywała na szybkość szukania minimum.\n","\n","### AdaGrad\n","Wprowadza mechanike **cache**, polega ona na tym że sumujemy to jaki wpływ miała zmiana danyj cechy na procesz uczenia i w zależności od tego zmieniamy ich learning rate.\n","![picture](https://drive.google.com/uc?id=1kVx1Cqx3KWZGoS8zZ3gynaq2AJ1XXdHb)\n","\n","### RMSProp\n","Z powodu tego że a AdaGrad zbyt szybko zmniejszamy learning rate przez co model przestawał się uczyć zbyt wczesnie, zmodyfikowano tą metodo w taki sposób że każda kolejna iteracja prowadza mniejsze zmiany."],"metadata":{"id":"XJst-ufnuLxA"}},{"cell_type":"markdown","source":["Adam składa łączy w sobie mechanizm momentu oraz learning rate shaduling"],"metadata":{"id":"g3U4eqb_OBZ4"}},{"cell_type":"markdown","source":[],"metadata":{"id":"j1PXUMRcOBXy"}},{"cell_type":"markdown","source":[],"metadata":{"id":"IGUIw-MLOBI_"}}]}