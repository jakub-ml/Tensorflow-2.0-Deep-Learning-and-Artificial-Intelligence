{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSxPSilRfCKMa+uNfhowKe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transfer Learning\n","### Transfer learning pozwala na szybkie douczanie modeli tak aby były przestosowane do naszych zadań, dzięki temu że korzystamy z już nauczonych modeli, proces ten będzie wymagał mniej danych oraz będzie trwał krócej niż uczenie modelu od poczatku.\n","![picture](https://drive.google.com/uc?id=1ZgeVZeoTEc-anX4-6WFJh_EYDeoRzkmT)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","### Metoda ta polega na tym że mamy model, przykładowo CNN. Ten model składa się z częsci konwolucyjnej której wagi zamrażamy, oraz części FNN której wagi będziemy zamieniać. Część konwolucyjna będzie niezmieniona ponieważ ta warstwa odpowiada za określanie cech. Pierwsze warstwy określają proste linie, kolejne bardziej złożone kształty (oko, ręka), a ostatnie całe struktury (osoby, budynki, samochody), na samym końcu otrzymujemy wektor okreslający właśnie te cechy. Dzięki temu jeśli modele były ćwiczone na ogromnych zbiorach danych wiemy że ta część robi to bardzo dobrze, więc my nie musimy się tym zajmować. Kolejna część - FNN, jest odpowiedzialna żeby za pomocą tych cech ostatecznie powiedzieć co widzimy na obrazie, jeśli docelowo chcemy rozpoznawać czy na obrazie widzimy psa czy kota to sieć neuronowa która przewiduje 10 klas jest dla nas bezużyteczna, dlatego właśnie tą cześć sieci będziemy trenowali - tak aby zmienić jej wagi oraz strukture sieci tak aby teraz pasowała do naszego zagadnienia. Przykładowo jeśli poczatkowy model miał przewidywać 10 klas, to zmieniamy FNN tak żeby przewidywał 2, trenujemy go na naszych danych i mamy nowy model, spełniający nasze działanie. Taki model ma już bardzo dobrze działającą cześć konwolucjną (której w praktyce lepiej byśmy pewnie nie wytrenowali) więc uczenie zajmuje mniej czas i potrzeba mniej danych.\n","![picture](https://drive.google.com/uc?id=1ZQmCIaXQCgZAC439NGbZ6pnrl5LTxn_Z)\n","\n","### Zamrażanie i trenowanie określonych części modelu.\n","![picture](https://drive.google.com/uc?id=1YT9BDvDKea14DGuWm4E1Z4fA58JEUuD5)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"JUPL_Wdnmjv3"}},{"cell_type":"markdown","source":["# **Typy CNN**\n","#### głównie chodzi o to kto zmieści więcej warst konwolucyjnych jednocześnie nie wpadając na problemy z długością uczenia modelu i vanishing gradient problem.\n","### **VGG16, VGG19** - numer odpowiada liczbie warstw, kiedyś bardzo mocna architektura dzięki wykorzystaniu na tamte czasy dużej ilości sieci\n","![picture](https://drive.google.com/uc?id=14quGXunJyud4qtZqZG8COqzhFqjIcDOj)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","### **ResNet** - CNN o dużej ilości warst - nawet 150. W CNN też występuje zjawisko vanishing gradient problem, przez co liczba warst konwolucnyjcych zazwyczaj sięgała do kilkunastu, dzięki mechanizmowi przeskakiwania danych przez niektóre warstwy ResNet mógł osiągać duże liczby warstw i dzięki temu jest jednym z mocniejszych modeli.\n","![picture](https://drive.google.com/uc?id=1YVLiaZsC8z8wkiO0iINImNT82fIgFrMN)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","### **Inception** - polega na tym że zamiast wybierać rozmiar filtra sprawdzamy każdy, tworzymy przez to równoległe gałęzie.\n","![picture](https://drive.google.com/uc?id=1iTUpz9QxDuq7q3TGzKzR49NRyxJvBI4b)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","### **MobileNet** - modele przystosowane do urządzeń z mniejszymi mocami obliczeniowymi - telefony, systemy wbudowane.\n","![picture](https://drive.google.com/uc?id=1QBPfbyOpOzmbmfbs2MxFbB12QC6RQgwk)\n","\n","\n","\n"],"metadata":{"id":"6WUF_88qCT_R"}},{"cell_type":"markdown","source":["# **Pojeścia do transfer learning**\n","### Przez to że w dużym modelach warstw konwolucyjnych potrafi być dużo, uczenie modelu przechodząc przez tą część trawałoby bardzo długo, biorąc pod uwagę że nasz model bedziemy chcieli uczyć więcej niż raz, będziemy chcieli przepuścić nasze obrazy przez częśc konwolucyjną aby za każdym kolejnym razem uczyć tylko cześć FNN za pomocą wektorów cech. Następnie możemy uczyć z lub bez augmentacji, jeśli uczymy z aumentacja to też będziemy musieli stworzyć odpowiednie wektory cech zanim zaczniemy uczyc model.\n","\n"],"metadata":{"id":"4qTj3VGdHqic"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFDsGwEMmhF3"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# Uczenie z augmentacją danych i bez feature transformation\n","### Zajmuje o wiele dłużej bez feature transofrmation i z augmentation"],"metadata":{"id":"tL901KdkOW4Z"}},{"cell_type":"code","source":[],"metadata":{"id":"awslbJfMOWrX","executionInfo":{"status":"ok","timestamp":1719065432760,"user_tz":-120,"elapsed":291,"user":{"displayName":"Jakub Mieszczak","userId":"11335813045427587721"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f8H4tpToOUY7","executionInfo":{"status":"ok","timestamp":1719065425049,"user_tz":-120,"elapsed":288,"user":{"displayName":"Jakub Mieszczak","userId":"11335813045427587721"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A7wVS08QOtNy","executionInfo":{"status":"ok","timestamp":1719065426646,"user_tz":-120,"elapsed":2,"user":{"displayName":"Jakub Mieszczak","userId":"11335813045427587721"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TdUjauJEOt1m","executionInfo":{"status":"ok","timestamp":1719065428569,"user_tz":-120,"elapsed":1,"user":{"displayName":"Jakub Mieszczak","userId":"11335813045427587721"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fksJFQlhOtzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qitMqa_ZOtwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"3FOsePIxOtud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FAOz0blmOtsH"},"execution_count":null,"outputs":[]}]}